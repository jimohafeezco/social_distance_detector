{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "social_dostancing_detector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnTijhepk4lS",
        "outputId": "50489f9d-fe43-48d5-acd3-6e1d4c3cb225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install imutils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9_kfu9rJIx",
        "outputId": "67ca1c3e-dfaf-40bb-c074-bc6c8fd8234f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "!wget https://github.com/RufinaMay/CV2019Fall_Pictures/raw/master/deploy.prototxt.txt\n",
        "!wget https://github.com/RufinaMay/CV2019Fall_Pictures/raw/master/res10_300x300_ssd_iter_140000.caffemodel\n",
        "# clear_output()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 03:37:32--  https://github.com/RufinaMay/CV2019Fall_Pictures/raw/master/deploy.prototxt.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/RufinaMay/CV2019Fall_Pictures/master/deploy.prototxt.txt [following]\n",
            "--2020-10-19 03:37:32--  https://raw.githubusercontent.com/RufinaMay/CV2019Fall_Pictures/master/deploy.prototxt.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28092 (27K) [text/plain]\n",
            "Saving to: ‘deploy.prototxt.txt.1’\n",
            "\n",
            "\rdeploy.prototxt.txt   0%[                    ]       0  --.-KB/s               \rdeploy.prototxt.txt 100%[===================>]  27.43K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-10-19 03:37:32 (2.19 MB/s) - ‘deploy.prototxt.txt.1’ saved [28092/28092]\n",
            "\n",
            "--2020-10-19 03:37:32--  https://github.com/RufinaMay/CV2019Fall_Pictures/raw/master/res10_300x300_ssd_iter_140000.caffemodel\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/RufinaMay/CV2019Fall_Pictures/master/res10_300x300_ssd_iter_140000.caffemodel [following]\n",
            "--2020-10-19 03:37:32--  https://raw.githubusercontent.com/RufinaMay/CV2019Fall_Pictures/master/res10_300x300_ssd_iter_140000.caffemodel\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10666211 (10M) [application/octet-stream]\n",
            "Saving to: ‘res10_300x300_ssd_iter_140000.caffemodel.1’\n",
            "\n",
            "res10_300x300_ssd_i 100%[===================>]  10.17M  19.6MB/s    in 0.5s    \n",
            "\n",
            "2020-10-19 03:37:33 (19.6 MB/s) - ‘res10_300x300_ssd_iter_140000.caffemodel.1’ saved [10666211/10666211]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x2q-N6HNtU2"
      },
      "source": [
        "# base path to YOLO directory\n",
        "MODEL_PATH = \"yolo-coco\"\n",
        "\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "USE_GPU = False\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "# set inline plots size\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 10) # (w, h)\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "import tarfile\n",
        "from collections import namedtuple\n",
        "from google.colab.patches import cv2_imshow\n",
        "from xml.dom import minidom"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdrMAE5_OWaD",
        "outputId": "12564926-8b9a-4509-d88b-e6cf5ef825e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-yolo/yolo-object-detection.zip?__s=1essnpgyhz7jwwcpjszi -O yolo-object-detection.zip\n",
        "# clear_output()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 03:37:43--  https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-yolo/yolo-object-detection.zip?__s=1essnpgyhz7jwwcpjszi\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.205.168\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.205.168|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308184552 (294M) [application/zip]\n",
            "Saving to: ‘yolo-object-detection.zip’\n",
            "\n",
            "yolo-object-detecti 100%[===================>] 293.91M  36.6MB/s    in 8.6s    \n",
            "\n",
            "2020-10-19 03:37:52 (34.1 MB/s) - ‘yolo-object-detection.zip’ saved [308184552/308184552]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7fTLVsO0iN"
      },
      "source": [
        "# UNZIP YOLO\n",
        "with zipfile.ZipFile('yolo-object-detection.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('yolo_data')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ok_coUOsrt",
        "outputId": "517a1f85-a8c7-4adc-978b-9e49e2351c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "scale = 0.00392\n",
        "classes_file = \"/content/yolo_data/yolo-object-detection/yolo-coco/coco.names\"\n",
        "weights = \"/content/yolo_data/yolo-object-detection/yolo-coco/yolov3.weights\"\n",
        "config_file = \"/content/yolo_data/yolo-object-detection/yolo-coco/yolov3.cfg\"\n",
        "classes = None\n",
        "LABELS = open(classes_file).read().strip().split(\"\\n\")\n",
        "\n",
        "with open(classes_file, 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# read pre-trained model and config file\n",
        "net = cv2.dnn.readNet(weights, config_file)\n",
        "LABELS"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['person',\n",
              " 'bicycle',\n",
              " 'car',\n",
              " 'motorbike',\n",
              " 'aeroplane',\n",
              " 'bus',\n",
              " 'train',\n",
              " 'truck',\n",
              " 'boat',\n",
              " 'traffic light',\n",
              " 'fire hydrant',\n",
              " 'stop sign',\n",
              " 'parking meter',\n",
              " 'bench',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'dog',\n",
              " 'horse',\n",
              " 'sheep',\n",
              " 'cow',\n",
              " 'elephant',\n",
              " 'bear',\n",
              " 'zebra',\n",
              " 'giraffe',\n",
              " 'backpack',\n",
              " 'umbrella',\n",
              " 'handbag',\n",
              " 'tie',\n",
              " 'suitcase',\n",
              " 'frisbee',\n",
              " 'skis',\n",
              " 'snowboard',\n",
              " 'sports ball',\n",
              " 'kite',\n",
              " 'baseball bat',\n",
              " 'baseball glove',\n",
              " 'skateboard',\n",
              " 'surfboard',\n",
              " 'tennis racket',\n",
              " 'bottle',\n",
              " 'wine glass',\n",
              " 'cup',\n",
              " 'fork',\n",
              " 'knife',\n",
              " 'spoon',\n",
              " 'bowl',\n",
              " 'banana',\n",
              " 'apple',\n",
              " 'sandwich',\n",
              " 'orange',\n",
              " 'broccoli',\n",
              " 'carrot',\n",
              " 'hot dog',\n",
              " 'pizza',\n",
              " 'donut',\n",
              " 'cake',\n",
              " 'chair',\n",
              " 'sofa',\n",
              " 'pottedplant',\n",
              " 'bed',\n",
              " 'diningtable',\n",
              " 'toilet',\n",
              " 'tvmonitor',\n",
              " 'laptop',\n",
              " 'mouse',\n",
              " 'remote',\n",
              " 'keyboard',\n",
              " 'cell phone',\n",
              " 'microwave',\n",
              " 'oven',\n",
              " 'toaster',\n",
              " 'sink',\n",
              " 'refrigerator',\n",
              " 'book',\n",
              " 'clock',\n",
              " 'vase',\n",
              " 'scissors',\n",
              " 'teddy bear',\n",
              " 'hair drier',\n",
              " 'toothbrush']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5KItuy5uSM-"
      },
      "source": [
        "# Easy to use car detector that takes an image and ouputs a list of bounding boxes\n",
        "class PersonDetector():\n",
        "  def __init__(self):\n",
        "    module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\"\n",
        "    self.detector = hub.load(module_handle).signatures['default']\n",
        "  \n",
        "  def filter_boxes(self, image, boxes, class_names, scores, max_boxes=10, min_score=0.6, class_of_interest='Person'):\n",
        "    im_height, im_width = image.shape[:2]\n",
        "    state=[]\n",
        "\n",
        "    for i in range(min(boxes.shape[0], max_boxes)):\n",
        "      if scores[i] >= min_score and class_names[i].decode(\"ascii\")==class_of_interest:\n",
        "        [ymin, xmin, ymax, xmax] = boxes[i]\n",
        "        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "        left, right, top, bottom = min(left, right), max(left, right), min(top, bottom), max(top, bottom)\n",
        "        left, right, top, bottom  = int(left), int(right), int(top), int(bottom) \n",
        "        state.append((left, top, right-left, bottom-top))\n",
        "    return state\n",
        "\n",
        "  def detect(self, img):\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "    result = self.detector(converted_img)\n",
        "\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "    bbox = self.filter_boxes(img, result[\"detection_boxes\"],\n",
        "        result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
        "    return bbox"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u70IKb2JPfEd"
      },
      "source": [
        "def detect_people(frame, net, ln, personIdx=0):\n",
        "\t# grab the dimensions of the frame and  initialize the list of\n",
        "\t# results\n",
        "\t(H, W) = frame.shape[:2]\n",
        "\tresults = []\n",
        "\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "\t\tswapRB=True, crop=False)\n",
        " \n",
        " \n",
        "\tnet.setInput(blob)\n",
        "\tlayerOutputs = net.forward(ln)\n",
        "\n",
        "\t# initialize our lists of detected bounding boxes, centroids, and\n",
        "\t# confidences, respectively\n",
        "\tboxes = []\n",
        "\tcentroids = []\n",
        "\tconfidences = []\n",
        "\n",
        "\t# loop over each of the layer outputs\n",
        "\tfor output in layerOutputs:\n",
        "\t\t# loop over each of the detections\n",
        "\t\tfor detection in output:\n",
        "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
        "\t\t\t# of the current object detection\n",
        "\t\t\tscores = detection[5:]\n",
        "\t\t\tclassID = np.argmax(scores)\n",
        "\t\t\tconfidence = scores[classID]\n",
        "\n",
        "\t\t\t# filter detections by (1) ensuring that the object\n",
        "\t\t\t# detected was a person and (2) that the minimum\n",
        "\t\t\t# confidence is met\n",
        "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
        "\t\t\t\t# scale the bounding box coordinates back relative to\n",
        "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
        "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
        "\t\t\t\t# the bounding box followed by the boxes' width and\n",
        "\t\t\t\t# height\n",
        "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
        "\t\t\t\t# and and left corner of the bounding box\n",
        "\t\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\t\ty = int(centerY - (height / 2))\n",
        "\n",
        "\t\t\t\t# update our list of bounding box coordinates,\n",
        "\t\t\t\t# centroids, and confidences\n",
        "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\t\tcentroids.append((centerX, centerY))\n",
        "\t\t\t\tconfidences.append(float(confidence))\n",
        "\n",
        "\t# apply non-maxima suppression to suppress weak, overlapping\n",
        "\t# bounding boxes\n",
        "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
        "\n",
        "\t# ensure at least one detection exists\n",
        "\tif len(idxs) > 0:\n",
        "\t\t# loop over the indexes we are keeping\n",
        "\t\tfor i in idxs.flatten():\n",
        "\t\t\t# extract the bounding box coordinates\n",
        "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "\t\t\t# update our results list to consist of the person\n",
        "\t\t\t# prediction probability, bounding box coordinates,\n",
        "\t\t\t# and the centroid\n",
        "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
        "\t\t\tresults.append(r)\n",
        "\treturn results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vg6UR7eT29T"
      },
      "source": [
        " def read_and_resize_image(filename, grayscale = False, fx= 1, fy=1):\n",
        "    if grayscale:\n",
        "      img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "      imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
        "      img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
        "    img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
        "    return img_result\n",
        "def showInRow(list_of_images, titles = None, disable_ticks = False):\n",
        "  count = len(list_of_images)\n",
        "  for idx in range(count):\n",
        "    subplot = plt.subplot(1, count, idx+1)\n",
        "    if titles is not None:\n",
        "      subplot.set_title(titles[idx])\n",
        "      \n",
        "    img = list_of_images[idx]\n",
        "    cmap = 'gray' if (len(img.shape) == 2 or img.shape[2] == 1) else None\n",
        "    subplot.imshow(img, cmap=cmap)\n",
        "    if disable_ticks:\n",
        "      plt.xticks([]), plt.yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDC3Jc9Pwra",
        "outputId": "eefb5588-2962-4e03-bf6a-c082f96c3cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy.spatial import distance as dist\n",
        "import imutils\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "ln\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yolo_82', 'yolo_94', 'yolo_106']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSuWHTh6a2Xn"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=view&id=1rHIUOstB87dtHDWgzbWhg5hmhqo-BHik\" -O pedestrians.mp4\n",
        "clear_output()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMv7Fde-URy9"
      },
      "source": [
        "# def process_video(video_path,out, num_frames=30, start_frame=0):]\n",
        "MIN_DISTANCE = 50\n",
        "video_path = \"/content/walk.mp4\"\n",
        "vid = cv2.VideoCapture(video_path)\n",
        "# vid.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "writer = None\n",
        "# try:\n",
        "while True:\n",
        "  ret, frame = vid.read()\n",
        "  if not ret:\n",
        "    vid.release()\n",
        "    break\n",
        "  frame = imutils.resize(frame, width=700)\n",
        "  # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  results = detect_people(frame, net, ln, personIdx=LABELS.index(\"person\"))\n",
        "  violate = set()\n",
        "\n",
        "  if len(results) >= 2:\n",
        "    centroids = np.array([r[2] for r in results])\n",
        "    D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "    # loop over the upper triangular of the distance matrix\n",
        "    for i in range(0, D.shape[0]):\n",
        "      for j in range(i + 1, D.shape[1]):\n",
        "        # check to see if the distance between any two\n",
        "        # centroid pairs is less than the configured number\n",
        "        # of pixels\n",
        "        if D[i, j] < MIN_DISTANCE:\n",
        "          # update our violation set with the indexes of\n",
        "          # the centroid pairs\n",
        "          violate.add(i)\n",
        "          violate.add(j)\n",
        "\n",
        "  for (i, (prob, bbox, centroid)) in enumerate(results):\n",
        "    (startX, startY, endX, endY) = bbox\n",
        "    (cX, cY) = centroid\n",
        "    color = (0, 255, 0)\n",
        "    # colorRed = (255,0, 0)\n",
        "    if i in violate:\n",
        "      color = (0, 0, 255)\n",
        "\n",
        "    cv2.circle(frame, (cX, cY), int((endY-startY)/2),color, 2)\n",
        "    cv2.circle(frame, (cX, cY), 5, color, 2)\n",
        "\n",
        "  text = \"Violations: {}\".format(len(violate))\n",
        "  cv2.putText(frame, text, (10, frame.shape[0] - 25),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "  # showInRow([frame])\n",
        "  if writer is  None:\n",
        "    writer =cv2.VideoWriter('dista_detector.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame.shape[1], frame.shape[0]),True)\n",
        "  # out.write(frame)\n",
        "  if writer is not None:\n",
        "      writer.write(frame)\n",
        "# except KeyboardInterrupt:\n",
        "  # vid.release()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}